{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>CSE3024 Lab 4: IR TF IDF </center>\n",
    "<h3 align=\"right\">Faraz Suhail</h3> \n",
    "<h3 align=\"right\">19BCE1525</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ques 1.\tConsider the following documents.\n",
    "\n",
    "Doc 1 : Information Retrieval Systems is used with database systems\n",
    "\n",
    "Doc 2 : Information is in Storage Storage\n",
    "\n",
    "Doc 3 : Digital Speech systems can be used in Synthesis and Systems\n",
    "\n",
    "Doc 4 : Speech Filtering, Speech Retrieval systems are applications of Information Retrieval\n",
    "\n",
    "Doc 5: Database Management system is used for storage storage\n",
    "\n",
    "i. Perform the text pre-processing of the given documents.\n",
    "\n",
    "ii. Construct a Boolean Model for the vocabulary list by considering documents 1, 2, 3,4 and 5.\n",
    "\n",
    "a. Retrieve the documents for the Boolean query “Information Retrieval Synthesis” using simple match.\n",
    "\n",
    "b. Retrieve the documents for the Boolean query “Database Retrieval Storage” using weighted match. (Rank the documents in the order of relevance)\n",
    "\n",
    "iii. Construct a vector space model to build the term weights. Compute the TF-IDF and identify the most important terms across the documents.\n",
    "\n",
    "a. Rank all the documents in the collection for the query “Speech” AND “Systems”? (Rank the Top 3 documents in the order of relevance)\n",
    "\n",
    "b. Rank all the documents in the collection for the query “Database” OR “Systems”? (Rank the Top N documents in the order of relevance)\n",
    "\n",
    "c. Rank all the documents in the collection for the query contains “Systems” but NOT “Information” (Rank the documents in the order of relevance)\n",
    "\n",
    "iv. Compute the cosine similarities between docs 1 and docs 2\n",
    "\n",
    "v. Compute Dice Co-efficient between docs 3 and docs 4.\n",
    "\n",
    "vi. Compute the Jaccard co-efficient between docs 4 and docs 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\faraz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\faraz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query:Information Retrieval AND Synthesis\n",
      "['and']\n",
      "information  not found\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\faraz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "all_words = []\n",
    "dict_global = {}\n",
    "file_folder = 'sample_data/doc/*'\n",
    "idx = 1\n",
    "files_with_index = {}\n",
    "\n",
    "for file in glob.glob(file_folder):\n",
    "\tprint(file)\n",
    "\tfname = file\n",
    "\tfile = open(file , \"r\")\n",
    "\ttext = file.read()\n",
    "\ttext = remove_special_characters(text)\n",
    "\ttext = re.sub(re.compile('\\d'),'',text)\n",
    "\tsentences = sent_tokenize(text)\n",
    "\twords = word_tokenize(text)\n",
    "\twords = [word for word in words if len(words)>1]\n",
    "\twords = [word.lower() for word in words]\n",
    "\twords = [word for word in words if word not in Stopwords]\n",
    "\tdict_global.update(finding_all_unique_words_and_freq(words))\n",
    "\tfiles_with_index[idx] = os.path.basename(fname)\n",
    "\tidx = idx + 1\n",
    "\n",
    "unique_words_all = set(dict_global.keys())\n",
    "\n",
    "def finding_all_unique_words_and_freq(words):\n",
    "\twords_unique = []\n",
    "\tword_freq = {}\n",
    "\tfor word in words:\n",
    "\t\tif word not in words_unique:\n",
    "\t\t\twords_unique.append(word)\n",
    "\tfor word in words_unique:\n",
    "\t\tword_freq[word] = words.count(word)\n",
    "\treturn word_freq\n",
    "\n",
    "\n",
    "def finding_freq_of_word_in_doc(word,words):\n",
    "\tfreq = words.count(word)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "\tregex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "\ttext_returned = re.sub(regex,'',text)\n",
    "\treturn text_returned\n",
    "\n",
    "\n",
    "class Node:\n",
    "\tdef init (self ,docId, freq = None):\n",
    "\t\tself.freq = freq\n",
    "\t\tself.doc = docId\n",
    "\t\tself.nextval = None\n",
    "\n",
    "class SlinkedList:\n",
    "\tdef init (self ,head = None):\n",
    "\t\tself.head = head\n",
    "\n",
    "linked_list_data = {}\n",
    "for word in unique_words_all:\n",
    "\tlinked_list_data[word] = SlinkedList()\n",
    "\tlinked_list_data[word].head = Node(1,Node)\n",
    "\n",
    "word_freq_in_doc = {}\n",
    "idx = 1\n",
    "for file in glob.glob(file_folder):\n",
    "\tfile = open(file, \"r\")\n",
    "\ttext = file.read()\n",
    "\ttext = remove_special_characters(text)\n",
    "\ttext = re.sub(re.compile('\\d'),'',text)\n",
    "\tsentences = sent_tokenize(text)\n",
    "\twords = word_tokenize(text)\n",
    "\twords = [word for word in words if len(words)>1]\n",
    "\twords = [word.lower() for word in words]\n",
    "\twords = [word for word in words if word not in Stopwords]\n",
    "\tword_freq_in_doc = finding_all_unique_words_and_freq(words)\n",
    "\tfor word in word_freq_in_doc.keys():\n",
    "\t\tlinked_list = linked_list_data[word].head\n",
    "\t\twhile linked_list.nextval is not None:\n",
    "\t\t\tlinked_list = linked_list.nextval\n",
    "\t\tlinked_list.nextval = Node(idx ,word_freq_in_doc[word])\n",
    "\tidx = idx + 1\n",
    "query = input('Enter your query:')\n",
    "query = word_tokenize(query)\n",
    "connecting_words = []\n",
    "cnt = 1\n",
    "different_words = []\n",
    "for word in query:\n",
    "\tif word.lower() != \"and\" and word.lower() != \"or\" and word.lower() != \"not\":\n",
    "\t\tdifferent_words.append(word.lower())\n",
    "\telse:\n",
    "\t\tconnecting_words.append(word.lower())\n",
    "print(connecting_words)\n",
    "total_files = len(files_with_index)\n",
    "zeroes_and_ones = []\n",
    "zeroes_and_ones_of_all_words = []\n",
    "for word in (different_words):\n",
    "\tif word.lower() in unique_words_all:\n",
    "\t\tzeroes_and_ones = [0] * total_files\n",
    "\t\tlinkedlist = linked_list_data[word].head\n",
    "\t\tprint(word)\n",
    "\t\twhile linkedlist.nextval is not None:\n",
    "\t\t\tzeroes_and_ones[linkedlist.nextval.doc - 1] = 1\n",
    "\t\t\tlinkedlist = linkedlist.nextval\n",
    "\t\tzeroes_and_ones_of_all_words.append(zeroes_and_ones)\n",
    "\telse:\n",
    "\t\tprint(word,\" not found\")\n",
    "\t\tsys.exit()\n",
    "print(zeroes_and_ones_of_all_words)\n",
    "for word in connecting_words:\n",
    "\tword_list1 = zeroes_and_ones_of_all_words[0]\n",
    "\tword_list2 = zeroes_and_ones_of_all_words[1]\n",
    "\tif word == \"and\":\n",
    "\t\tbitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,word_list2)]\n",
    "\t\tzeroes_and_ones_of_all_words.remove(word_list1)\n",
    "\t\tzeroes_and_ones_of_all_words.remove(word_list2)\n",
    "\t\tzeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "\telif word == \"or\":\n",
    "\t\tbitwise_op = [w1 | w2 for (w1,w2) in zip(word_list1,word_list2)]\n",
    "\t\tzeroes_and_ones_of_all_words.remove(word_list1)\n",
    "\t\tzeroes_and_ones_of_all_words.remove(word_list2)\n",
    "\t\tzeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "\telif word == \"not\":\n",
    "\t\tbitwise_op = [not w1 for w1 in word_list2]\n",
    "\t\tbitwise_op = [int(b == True) for b in bitwise_op]\n",
    "\t\tzeroes_and_ones_of_all_words.remove(word_list2)\n",
    "\t\tzeroes_and_ones_of_all_words.remove(word_list1)\n",
    "\t\tbitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,bitwise_op)]\n",
    "\t\tzeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "\n",
    "files = []\n",
    "print(zeroes_and_ones_of_all_words)\n",
    "lis = zeroes_and_ones_of_all_words[0]\n",
    "cnt = 1\n",
    "for index in lis:\n",
    "\tif index == 1:\n",
    "\t\tfiles.append(files_with_index[cnt])\n",
    "\tcnt = cnt+1\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: Systems, TF-IDF: 0.12771\n",
      "\tWord: with, TF-IDF: 0.11454\n",
      "\tWord: database, TF-IDF: 0.11454\n",
      "Top words in document 2\n",
      "\tWord: Storage, TF-IDF: 0.36652\n",
      "\tWord: in, TF-IDF: 0.10217\n",
      "\tWord: Information, TF-IDF: 0.04463\n",
      "Top words in document 3\n",
      "\tWord: Systems, TF-IDF: 0.10217\n",
      "\tWord: Digital, TF-IDF: 0.09163\n",
      "\tWord: can, TF-IDF: 0.09163\n",
      "Top words in document 4\n",
      "\tWord: Speech, TF-IDF: 0.10217\n",
      "\tWord: Retrieval, TF-IDF: 0.10217\n",
      "\tWord: Filtering, TF-IDF: 0.09163\n",
      "Top words in document 5\n",
      "\tWord: storage, TF-IDF: 0.22907\n",
      "\tWord: Database, TF-IDF: 0.11454\n",
      "\tWord: Management, TF-IDF: 0.11454\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "from textblob import TextBlob as tb\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "document1 = tb(\"\"\"Information Retrieval Systems is used with database systems\"\"\")\n",
    "document2 = tb(\"\"\"Information is in Storage Storage\"\"\")\n",
    "document3 = tb(\"\"\"Digital Speech systems can be used in Synthesis and Systems \"\"\")\n",
    "document4 = tb(\"\"\"Speech Filtering, Speech Retrieval systems are applications of Information Retrieval \"\"\")\n",
    "document5 = tb(\"\"\"Database Management system is used for storage storage\"\"\")\n",
    "bloblist = [document1, document2, document3, document4, document5]\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score,5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: \n",
      "[[1.         0.23904572]\n",
      " [0.23904572 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Define the documents\n",
    "doc1 = \"Information Retrieval Systems is used with database systems\"\n",
    "doc2 = \"Information is in Storage Storage\"\n",
    "documents = [doc1, doc2]\n",
    "# Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix,\n",
    " columns=count_vectorizer.get_feature_names(),  index=['doc1', 'doc2'])\n",
    "df\n",
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(\"Cosine Similarity: \")\n",
    "print(cosine_similarity(df, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Coefficient 0.41509433962264153\n"
     ]
    }
   ],
   "source": [
    "def dice_coefficient(a, b):\n",
    "    \"\"\"dice coefficient 2nt/(na + nb).\"\"\"\n",
    "    if not len(a) or not len(b): return 0.0\n",
    "    if len(a) == 1: a=a+u'.'\n",
    "    if len(b) == 1: b=b+u'.'\n",
    "    a_bigram_list=[]\n",
    "    for i in range(len(a)-1):\n",
    "        a_bigram_list.append(a[i:i+2])\n",
    "    b_bigram_list=[]\n",
    "    for i in range(len(b)-1):\n",
    "        b_bigram_list.append(b[i:i+2])\n",
    "    a_bigrams = set(a_bigram_list)\n",
    "    b_bigrams = set(b_bigram_list)\n",
    "    overlap = len(a_bigrams & b_bigrams)\n",
    "    dice_coeff = overlap * 2.0/(len(a_bigrams) + len(b_bigrams))\n",
    "    return dice_coeff\n",
    "\n",
    "doc3 = \"Digital Speech systems can be used in Synthesis and Systems\"\n",
    "doc4 = \"Speech Filtering, Speech Retrieval systems are applications of Information Retrieval\"\n",
    "result = dice_coefficient(doc3,doc4)\n",
    "print(\"Dice Coefficient\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard co-efficient: 0.125\n"
     ]
    }
   ],
   "source": [
    "doc4 =\"Speech Filtering, Speech Retrieval systems are applications of Information Retrieval \"\n",
    "doc5 =\"Digital Speech systems can be used in Synthesis and Systems\"\n",
    "def get_jaccard_sim(str1, str2):\n",
    "    a = set(str1.split())\n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "result = get_jaccard_sim(doc4,doc5)\n",
    "print(\"Jaccard co-efficient:\" ,result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
